{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - Python Code and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: extract images from zip file into train folder in the base of the current working directory of this notebook, this can be done manually or via zipfile. Example:\n",
    "<pre>\n",
    "import zipfile\n",
    "with zipfile.ZipFile('train.zip','r') as z:\n",
    "    z.extractall(\"train\")\n",
    "    print('The train dataset is extracted into the train folder of the current working directory')\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Extracted Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import extracted training files, in this case files were extracted to the train folder in my current working directory\n",
    "Train_Path = \"train\"\n",
    "train_files = os.listdir(Train_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['cat.0.jpg',\n",
       " 'cat.1.jpg',\n",
       " 'cat.10.jpg',\n",
       " 'cat.100.jpg',\n",
       " 'cat.1000.jpg',\n",
       " 'cat.10000.jpg',\n",
       " 'cat.10001.jpg',\n",
       " 'cat.10002.jpg',\n",
       " 'cat.10003.jpg',\n",
       " 'cat.10004.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of training images\n",
    "len(train_files)\n",
    "\n",
    "# first ten image file names\n",
    "train_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Labels from File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10001</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10002</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10003</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10004</td>\n",
       "      <td>cat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id label  label_num\n",
       "0      0   cat          1\n",
       "1      1   cat          1\n",
       "2     10   cat          1\n",
       "3    100   cat          1\n",
       "4   1000   cat          1\n",
       "5  10000   cat          1\n",
       "6  10001   cat          1\n",
       "7  10002   cat          1\n",
       "8  10003   cat          1\n",
       "9  10004   cat          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# extract label from file name\n",
    "label = []\n",
    "identifier = []\n",
    "for file in train_files:\n",
    "    file_name = file.split(\".\")\n",
    "    label.append(file_name[0])\n",
    "    identifier.append(file_name[1])\n",
    "\n",
    "# create df with id and label\n",
    "train_df = pd.DataFrame(data={'id':identifier,'label':label})\n",
    "\n",
    "# dummy encode label column\n",
    "train_df[\"label_num\"] = np.where(train_df[\"label\"] == 'cat', 1, 0)\n",
    "\n",
    "# number of labels should be 25000\n",
    "train_df.shape\n",
    "\n",
    "# first ten rows\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Cats and Dogs')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import plotly as plt\n",
    "sns.countplot(x=train_df.label).set(title = 'Cats and Dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Dimensions from First 100 Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(374, 500, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(280, 300, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(499, 489, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(499, 403, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(149, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(359, 431, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(374, 500, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(471, 499, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(375, 499, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(239, 320, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dimension\n",
       "0  (374, 500, 3)\n",
       "1  (280, 300, 3)\n",
       "2  (499, 489, 3)\n",
       "3  (499, 403, 3)\n",
       "4  (149, 150, 3)\n",
       "5  (359, 431, 3)\n",
       "6  (374, 500, 3)\n",
       "7  (471, 499, 3)\n",
       "8  (375, 499, 3)\n",
       "9  (239, 320, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# loop through training files to get image dimensions\n",
    "img=[]\n",
    "for file in train_files[0:100]:\n",
    "    count=+1\n",
    "    img.append(cv2.imread(os.path.join(Train_Path,file)).shape)\n",
    "\n",
    "# create df with dim\n",
    "dim_df = pd.DataFrame(data={'dimension':img})\n",
    "dim_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Extracted Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extracted testing files, in this case files were extracted to the test folder in my current working directory\n",
    "Test_Path = \"test\"\n",
    "test_files = os.listdir(Test_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['1.jpg',\n",
       " '10.jpg',\n",
       " '100.jpg',\n",
       " '1000.jpg',\n",
       " '10000.jpg',\n",
       " '10001.jpg',\n",
       " '10002.jpg',\n",
       " '10003.jpg',\n",
       " '10004.jpg',\n",
       " '10005.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of test images\n",
    "len(test_files)\n",
    "\n",
    "# first ten image file names\n",
    "test_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and Resize Training Images to 150x150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# make new directory for cropped pictures\n",
    "Train_Cropped_Path = \"train_cropped/\"\n",
    "os.mkdir(Train_Cropped_Path)\n",
    "\n",
    "# crop images and save in train_cropped folder\n",
    "for file in train_files:\n",
    "    im = Image.open(os.path.join(Train_Path,file))\n",
    "    im = im.resize((150, 150))\n",
    "    im = im.save(f\"{Train_Cropped_Path}crop{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move cropped images to 'cat' or 'dog' folder based on label for the image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, sys\n",
    "\n",
    "# image labels\n",
    "categories = ['cat' , 'dog']\n",
    "\n",
    "# function to move cat images and dog images to folders\n",
    "def move_images_to_specific_folder(new_path, category):\n",
    "    for image_name in os.listdir(new_path):\n",
    "        if category in image_name:\n",
    "            if image_name.endswith('.jpg'):\n",
    "                shutil.move(os.path.join(new_path,image_name), os.path.join(new_path, category))\n",
    "    \n",
    "# create folders for cats and dogs\n",
    "for category in categories:\n",
    "    path = os.path.join(Train_Cropped_Path, category)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# move cropped files to appropriate folder based on label\n",
    "for category in categories:\n",
    "    move_images_to_specific_folder(Train_Cropped_Path, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dimensions of Cropped Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(150, 150, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dimension\n",
       "0  (150, 150, 3)\n",
       "1  (150, 150, 3)\n",
       "2  (150, 150, 3)\n",
       "3  (150, 150, 3)\n",
       "4  (150, 150, 3)\n",
       "5  (150, 150, 3)\n",
       "6  (150, 150, 3)\n",
       "7  (150, 150, 3)\n",
       "8  (150, 150, 3)\n",
       "9  (150, 150, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through training files to get image dimensions\n",
    "Train_Cropped_Path = \"train_cropped/\"\n",
    "img=[]\n",
    "for category in categories:\n",
    "    path = os.path.join(Train_Cropped_Path, category)\n",
    "    train_cropped_files = os.listdir(path)\n",
    "    for file in train_cropped_files:\n",
    "        img.append(cv2.imread(os.path.join(path,file)).shape)\n",
    "\n",
    "# create df with dimensions, all images should be 150x150\n",
    "dim_cropped_df = pd.DataFrame(data={'dimension':img})\n",
    "dim_cropped_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are resized to 150x150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:49:39.354888: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 09:49:39.443942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 09:49:40.278528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# load all libraries needed\n",
    "import shutil, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create ImageDataGenerator to apply preprocessing for images and split data to batches\n",
    "image_size = 150\n",
    "batch_size = 10 # keep batch size small for now\n",
    "epochs = 10 # start with 10 epochs\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, # rescale the image pixels\n",
    "                                   rotation_range=20,\n",
    "                                   validation_split=0.2, # allocate 20% of the data as validation\n",
    "                                   horizontal_flip=True,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('train_cropped/', \n",
    "                                                    class_mode='binary', \n",
    "                                                    batch_size = batch_size, \n",
    "                                                    target_size=(image_size,image_size), \n",
    "                                                    subset='training', \n",
    "                                                    shuffle=True, \n",
    "                                                    seed=10)\n",
    "validation_generator = train_datagen.flow_from_directory('train_cropped/', \n",
    "                                                         class_mode='binary', \n",
    "                                                         batch_size = batch_size, \n",
    "                                                         target_size=(image_size,image_size), \n",
    "                                                         subset='validation', \n",
    "                                                         shuffle=True, \n",
    "                                                         seed=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STILL WORKING ON ADJUSTING MODEL PARAMETERS - WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 150, 150, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 75, 75, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 37, 37, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 37, 37, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 18, 18, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 41472)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 41473     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,257\n",
      "Trainable params: 136,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Initial Sequential Model\n",
    "model = Sequential()\n",
    "# 2D convolutional layer w/32 filters, 5x5 kernel, and ReLU activation function. Model expects 150x150 image with 3 channels.\n",
    "model.add(Conv2D(input_shape=(150,150,3), filters = 32, kernel_size=(5,5), strides=(1,1), padding=('same'), activation=\"relu\"))\n",
    "# max pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# Conv2D and MaxPooling2D layers with 64 filters and 3x3 kernel.\n",
    "model.add(Conv2D(filters = 64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# Conv2D and MaxPooling2D layers with 128 filters 3x3 kernel.\n",
    "model.add(Conv2D(filters = 128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten output of the previous layer\n",
    "model.add(Flatten())\n",
    "# add dense layer\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam solver optimizer with learning rate of 0.01\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# create model\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484/2143819318.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator, steps_per_epoch=train_generator.samples // batch_size,\n",
      "2023-05-19 10:06:51.972399: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-19 10:12:09.460272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69313, saving model to best_model.cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.cnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.cnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 - 386s - loss: 0.7016 - accuracy: 0.4981 - val_loss: 0.6931 - val_accuracy: 0.5000 - 386s/epoch - 19ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.69313\n",
      "20000/20000 - 339s - loss: 0.6945 - accuracy: 0.4956 - val_loss: 0.6936 - val_accuracy: 0.5000 - 339s/epoch - 17ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.69313\n",
      "20000/20000 - 318s - loss: 0.6946 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000 - 318s/epoch - 16ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69313\n",
      "20000/20000 - 321s - loss: 0.6945 - accuracy: 0.5028 - val_loss: 0.6936 - val_accuracy: 0.5000 - 321s/epoch - 16ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.69313\n",
      "20000/20000 - 328s - loss: 0.6942 - accuracy: 0.5029 - val_loss: 0.6970 - val_accuracy: 0.5000 - 328s/epoch - 16ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.69313\n",
      "20000/20000 - 325s - loss: 0.6946 - accuracy: 0.4956 - val_loss: 0.6945 - val_accuracy: 0.5000 - 325s/epoch - 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping based on validation loss (stops if doesn't improve after 5 iterations)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# save the best model as 'best_model.cnn' based on validation loss\n",
    "save_best = ModelCheckpoint(filepath = 'best_model.cnn', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit model using batches of training data and batches of testing data\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=train_generator.samples // batch_size, \n",
    "                              validation_data = validation_generator,\n",
    "                              validation_steps = validation_generator.samples // batch_size, \n",
    "                              epochs = epochs, \n",
    "                              callbacks=[save_best,early_stopping], \n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "best_model = tf.keras.models.load_model('best_model.cnn')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and Resize Testing Images to 150x150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new directory for cropped images\n",
    "Test_Cropped_Path = \"test_cropped\"\n",
    "os.mkdir(Test_Cropped_Path)\n",
    "\n",
    "# crop images and save in new folder\n",
    "for file in test_files:\n",
    "    im = Image.open(os.path.join(Test_Path,file))\n",
    "    im = im.resize((150, 150))\n",
    "    im.save('test_cropped/'+file)\n",
    "test_cropped_files = os.listdir(Test_Cropped_Path)\n",
    "\n",
    "\n",
    "# make new directory for cropped pictures\n",
    "Train_Cropped_Path = \"train_cropped/\"\n",
    "os.mkdir(Train_Cropped_Path)\n",
    "\n",
    "# crop images and save in train_cropped folder\n",
    "for file in train_files:\n",
    "    im = Image.open(os.path.join(Train_Path,file))\n",
    "    im = im.resize((150, 150))\n",
    "    im = im.save(f\"{Train_Cropped_Path}crop{file}\")\n",
    "\n",
    "test_df = pd.DataFrame(data = test_cropped_files, columns = ['filename'])\n",
    "test_df['id'] = test_df['filename'].apply(lambda f: int(f.split('.')[0]))\n",
    "test_df.sort_values(by = 'id', inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Test Data into Testing Generator for Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale image pixels\n",
    "test_gen = ImageDataGenerator(rescale = 1./255)\n",
    "# create test generator for testing data\n",
    "test_generator = test_gen.flow_from_dataframe(test_df, \n",
    "                                              'test', \n",
    "                                               x_col='filename',\n",
    "                                               class_mode= None,\n",
    "                                               target_size=(image_size,image_size),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions into CSV file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract id from file name\n",
    "identifier = []\n",
    "for file in test_files:\n",
    "    file_name = file.split(\".\")\n",
    "    identifier.append(file_name[0])\n",
    "\n",
    "# Apply the cnn model1 to the test dataset\n",
    "cnn_pred1 = best_model.predict(test_generator, verbose = 1)\n",
    "\n",
    "# Put the label predictions into a dataframe\n",
    "cnn_pred1_df = pd.DataFrame(cnn_pred1, columns=['Label'])\n",
    "\n",
    "# Add the ID column to the front of the cnn predictions dataframe\n",
    "cnn_pred1_df.insert(0, 'ImageId', identifier)\n",
    "\n",
    "# Output predictions to csv\n",
    "cnn_pred1_df.to_csv('test_predictions_cnn_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the Kaggle results from the application of the CNN model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the kaggle results associated with the MLP Classifier Model\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (15, 15))\n",
    "kaggle_results = plt.imread('Kaggle_results_cnn_v1.jpg')\n",
    "plt.imshow(kaggle_results)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
